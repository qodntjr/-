---딥러닝을 이용한 자연어 처리(NLP)

텍스트의 토큰화
text_to_word_sequence() 함수로 분리
from tensorflow.keras.preprocessing.text import text_to_word_sequence
텍스트를 단어 단위로 쪼개는 것은 가장 많이 쓰는 전처리이다
Tokenizer() 함수
from tensorflow.keras.preprocessing.text import Tokenizer

단어의 원-핫 인코딩
단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지 알아보기 위해 사용한다
각 단어를 모두 0으로 바꾸어 주고 원하는 단어만 1로 바꾸어 주는것이다

단어 임베딩
원-핫 인코딩을 그대로 사용하면 벡터의 길이가 너무 길어진다는 단점이 있다
공간적낭비를 해결하기 위해 사용하는 방법으로 주어진 배열을 정해진 길이로 압축시킨다.
Embedding() 함수

텍스트를 읽고 긍정 부정 예측하기

패딩은 사이즈를 맞춰주는 역할이다
패딩은 자연어 처리뿐 아니라 GAN에서도 중요한 역할을 한다

--- 시퀀스 배열로 다루는 순환 신경망(RNN)

여러 개의 데이터가 순서대로 입력되었을 때 앞서 입력받은 데이터를 잠시 기억해 놓는 방법

LSTM - 기억 값의 가중치를 관리하는 장치

-----
수치 차이가 크면 학습이 잘 안될 수 있어서 스케일을 활용하여 학습시킨다